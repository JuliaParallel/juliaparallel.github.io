<!doctype html>

<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
   <link rel="stylesheet" href="/previews/PR40/libs/highlight/github.min.css">
   
  <script defer data-domain="juliaparallel.org" src="https://plausible.io/js/plausible.js"></script>
  <link rel="stylesheet" href="/previews/PR40/css/franklin.css">
<link rel="stylesheet" href="/previews/PR40/css/minimal-mistakes.css">
<link rel="stylesheet" href="/previews/PR40/css/adjust.css">
<link rel="icon" href="/previews/PR40/assets/favicon.png">


   <title>Comments on the Julius Graph Engine Benchmark</title>  
  
  
</head>
<body class="layout--single">
  <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <img class="jp-logo" src="/previews/PR40/assets/logo.png"/>
        <a class="site-title" href="/previews/PR40/">JuliaParallel</a>
        <ul class="visible-links">
          <li class="masthead__menu-item"><a href="/previews/PR40/news/">News</a></li>
          <li class="masthead__menu-item"><a href="/previews/PR40/resources/">Resources</a></li>
          <li class="masthead__menu-item"><a href="/previews/PR40/tutorials/">Tutorials</a></li>
          <li class="masthead__menu-item"><a href="https://github.com/JuliaParallel">Github</a></li>
        </ul>
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

  <div class="initial-content">
    <div id="main" role="main">



<div class="franklin-content" >
  <h1 id="comments_on_the_julius_graph_engine_benchmark" ><a href="#comments_on_the_julius_graph_engine_benchmark"> Comments on the Julius Graph Engine Benchmark</a></h1><div class="toc"><ol><li><a href="#introduction">Introduction</a></li><li><a href="#the_julius_scheduler_benchmark">The Julius Scheduler Benchmark</a><ol><li><a href="#aside_scheduling_for_graph_engines">Aside: Scheduling for Graph Engines</a></li></ol></li><li><a href="#benchmark_results_prelude_and_interpretation">Benchmark Results - Prelude and Interpretation</a><ol><li><a href="#comparison_what_graph_building_modes_are_supported?">Comparison: What graph-building modes are supported?</a></li></ol></li><li><a href="#benchmark_results">Benchmark Results</a><ol><li><a href="#comparison_initial_results_on_s_n_(in_seconds)">Comparison: Initial results on <code>s_n</code> (in seconds)</a></li><li><a href="#comparison_initial_results_on_y_n_(in_seconds)">Comparison: Initial results on <code>y_n</code> (in seconds)</a></li></ol></li><li><a href="#oh_no!_can_we_fix_it?">Oh No! Can we fix it?</a></li><li><a href="#first_fix_object_size_calculation">First Fix: Object Size Calculation</a></li><li><a href="#second_fix_node_memoization">Second Fix: Node Memoization</a></li><li><a href="#third_fix_domination_checks">Third Fix: Domination Checks</a></li><li><a href="#giving_credit_where_credit_is_due">Giving credit where credit is due</a></li><li><a href="#debrief">Debrief</a></li><li><a href="#conclusion">Conclusion</a><ol><li><a href="#comparison_important_graph_scheduler_features">Comparison: Important graph scheduler features</a></li></ol></li><li><a href="#aside_future_work_and_collaboration">Aside: Future work and collaboration</a></li><li><a href="#contact_information">Contact Information</a></li></ol></div><h3 id="introduction" ><a href="#introduction"> Introduction</a></h3><br><img src="https://thumbnails.production.thenounproject.com/GZSNSq5eKQqKDoHfKGOFmb5QT4s=/fit-in/1000x1000/photos.production.thenounproject.com/photos/6D93E0C1-DFF3-410B-8086-214D12A2D362.jpg" alt=""Shipping goods over the sea via ocean freighter."">

<p style="text-align: center;">
Public domain image courtesy of <a href="https://thenounproject.com/carolhighsmith/">carolhighsmith</a>
</p><p>Scheduling is a hard problem, but it's a necessary evil for modern civilization:</p>
<ul>
<li>Moving perishable goods quickly from producer to consumer</li>
<li>Tracking and re-routing aircraft to prevent collisions while optimizing flight paths</li>
<li>Planning traffic light timings to avoid gridlock</li>
</ul>
<p>In the space of computing, we often have lots of different kinds of tasks we need to complete, but only a few computing resources to fulfill these tasks. There are plenty of naive ways to do scheduling, from round-robin to FIFO and LIFO, and these can work out well when your tasks and computing resources are homogeneous. But life is rarely so simple; tasks can be small or large and can have dependencies on each other, and computing resources aren't all alike (e.g. CPU vs. GPU). Using a naive scheduling algorithm for such problems is a recipe for sitting around, waiting for an analysis or computation to complete. Thankfully, there exist smarter schedulers, like Dask, Ray, and Dagger.jl, which are able to handle heterogeneous task scheduling effectively through resource queries, runtime metric collection, and other smart ideas.</p>
<p>In this blog post, we'll introduce you to a benchmark of some of these schedulers, and walk you through how I optimized Dagger.jl's scheduler to more efficiently run the benchmark. I'll also introduce you to a new proprietary scheduler platform offered by a Julia-focused startup, and show how it stacks up against the open-source schedulers. I'll finally give some ideas for problems that these schedulers can solve effectively, which will help you understand how these schedulers can support your computational needs.</p>
<h3 id="the_julius_scheduler_benchmark" ><a href="#the_julius_scheduler_benchmark"> The Julius Scheduler Benchmark</a></h3><p>In the last few weeks, it came to my attention that <a href="https://www.juliustech.co/">Julius Technologies</a>, a Julia-focused startup, published a <a href="https://juliustechco.github.io/JuliusGraph/dev/pages/t007_benchmark.html">benchmark</a> of their "Graph Engine", which is a proprietary low/no-code programming platform, likely written in Julia. They compared the performance of their platform on two kinds of benchmarks, and provided equivalent benchmark scripts written for Dask, TensorFlow, and Dagger.jl. The benchmark showed a very wide margin in runtimes between Julius Graph Engine (which I'll call "JGE") and the competition, with JGE coming in more than an order of magnitude faster, and scaling near-linearly. Notably, Dask and Dagger showed very poor performance, and weren't able to complete most of the benchmark, only working on smaller scales.</p>
<p>As the maintainer of Dagger.jl, I have skin in this benchmarking game. Most users of Dagger came to it under the premise and with the promise of fast heterogeneous programming. Since these results showed that Dagger struggles with executing certain common kinds of programs, I decided to spend a few days tweaking Dagger to get the performance that I’d want. All of the changes that I’ll be describing in this post are going upstream to Dagger in one way or another. In this blog post, I'll introduce you to graph engines and how their schedulers work, I'll talk about how I profiled Dagger's runtime costs, and I'll walk you through how I brought Dagger's benchmark runtime down to within 10x of Julius' product offering.</p>
<p>(Side Note: Julius has since updated their benchmark showing Dagger doing much better (thanks to improvements spurred by their benchmark), but I've kept my original benchmark results below for the purpose of explanation.)</p>
<h4 id="aside_scheduling_for_graph_engines" ><a href="#aside_scheduling_for_graph_engines"> Aside: Scheduling for Graph Engines</a></h4><p>Let's back up for a second: why should you care? What is a "graph engine", and what does it have to do with scheduling? Starting from the top: A "graph engine" is just a fancy way of talking about a program which executes code represented as a Directed Acyclic Graph, or DAG. Any program you've ever written can probably be represented as a DAG; the vertices of the DAG are typically basic operations, such as arithmetic or memory access, while the edges of the DAG represent calls between functions, or control flow like <code>for</code>-loops or <code>try-catch</code> blocks. For an example of this (using Dagger), see <a href="https://juliaparallel.org/Dagger.jl/dev/#Simple-example">this documentation section</a>.</p>
<p>You can even see this with regular Julia code: when Julia code is "lowered" by Julia's frontend, it's converted into a graph for later analysis and compilation, although it's not guaranteed to be acyclic (and if you write a <code>for</code> or <code>while</code> loop, it's definitely not acyclic). This lack of cyclicity can be worked around by "unrolling" a cyclic directed graph into an acyclic equivalent, and doing lots of copy-pasta of the code within each graph cycle. Importantly, this isn't a trivial thing to do <em>efficiently</em>, so it's still an active area of research and development for graph engines. The schedulers that underlie graph engines sometimes have built-in fast paths for such cases, but in the absence of those, having low scheduling overhead is paramount to acceptable performance.</p>
<p>What about all those other kinds of schedulers that I pointed out in the intro? Well, schedulers for other use cases don't necessarily compare well with graph schedulers, because they're solving fundamentally different problems, and thus doing different classes of scheduling. So for the rest of this post, all of the schedulers that we'll be looking at are designed for graph execution, so we can compare "apples to apples".</p>
<h3 id="benchmark_results_prelude_and_interpretation" ><a href="#benchmark_results_prelude_and_interpretation"> Benchmark Results - Prelude and Interpretation</a></h3><p>Anyway, that's enough background. Let's scrutinize this benchmark a bit more, because at first guess, we shouldn't expect a newcomer to the graph scheduling space to handily beat out two different production Python schedulers and a pure-Julia scheduler (and kudos to Julius for pulling that off). The benchmark has two parts, which they call <a href="https://juliustechco.github.io/JuliusGraph/dev/assets/widegraph.png"><code>s_n</code></a> and <a href="https://juliustechco.github.io/JuliusGraph/dev/assets/deepgraph.png"><code>y_n</code></a> (<a href="https://juliustechco.github.io/JuliusGraph/dev/pages/t007_benchmark.html#Benchmark-Setup-1">details here</a>). The <code>s_n</code> benchmark tests DAGs which are really "wide", which means that a single node has a lot of other directly-connected nodes. The <code>y_n</code> benchmark tests DAGs which are really "deep", which means that there is a really long path from start to finish (going through many different nodes). The core or "kernel" of each benchmark is a <code>fibsum</code> function, which is very cheap (two multiplies and one add per each of 10 array elements). This kind of setup is a pretty common way to stress-test graph schedulers, since it exposes the cost of every little part of scheduling that isn't directly executing the user's functions. In other words, it effectively exposes the overhead of the scheduler being used.</p>
<p>Something else that we need to understand about this benchmark is that the four graph schedulers included are not all alike. One of the most important ways that schedulers can be compared is "visibility"; can the scheduler see the entire DAG before it starts executing, or does it only get bits and pieces as it goes along executing? This is an important consideration because being able to see the full DAG means that it's easy to perform optimizations on it, such as combining repetitive sub-graphs with a <code>for</code>-loop (basically undoing the "unrolling" of the graph so that the language's compiler can better optimize the whole sub-graph). Constructing the whole graph also incurs memory overhead, because the entire graph needs to exist in memory at some point in time; in certain cases, it can be prohibitively expensive just to construct the graph (let alone to actually execute it).</p>
<p>From what I understand, the JGE requires visibility into the whole DAG before execution begins; the same is also true for Dask and TensorFlow. However, it's also possible to design a scheduler that instead only sees parts of the DAG, as it is being built. I will term these two modes "ahead-of-time" (AOT) and "just-in-time" (JIT), respectively (these are often also referred to as "static" and "dynamic", respectively). So what does Dagger do? Well, in the benchmark, Dagger is being used in JIT mode, although it also supports an AOT mode. JIT mode (using <code>@spawn</code> and <code>fetch</code>) is recommended for most users, as it is often easier to use, doesn't store the full graph in memory all at once, and doesn't require knowledge of the full graph before execution can begin. However, AOT mode (using <code>delayed</code> and <code>compute</code>) has the benefit of being very efficient at consuming a fully constructed DAG, and can use less memory at runtime for reasons I won't get into here.</p>
<h4 id="comparison_what_graph_building_modes_are_supported?" ><a href="#comparison_what_graph_building_modes_are_supported?"> Comparison: What graph-building modes are supported?</a></h4><table class="">
<thead>
<th style="text-align:left;"> Feature </th>
<th style="text-align:center;"> Dask </th>
<th style="text-align:center;"> Ray </th>
<th style="text-align:center;"> Dagger.jl </th>
<th style="text-align:center;"> JGE </th>
<th style="text-align:center;"> TensorFlow </th>
<th style="text-align:center;"> Cilk </th>
<th style="text-align:center;"> Vanilla Julia </th>
</thead>
<tbody>
<tr>
<td style="text-align:left;"> AOT (static) </td>
<td style="text-align:center;"> ✔ </td>
<td style="text-align:center;"> ✔ (Actors) </td>
<td style="text-align:center;"> ✔ (<code>delayed</code>) </td>
<td style="text-align:center;"> ✔ </td>
<td style="text-align:center;"> ✔ (TF 1.x) </td>
<td style="text-align:center;"> ❌ </td>
<td style="text-align:center;"> ❌ </td>
</tr>
<tr>
<td style="text-align:left;"> JIT (dynamic) </td>
<td style="text-align:center;"> ❌ </td>
<td style="text-align:center;"> ✔ (Tasks) </td>
<td style="text-align:center;"> ✔ (<code>@spawn</code>) </td>
<td style="text-align:center;"> ❌ </td>
<td style="text-align:center;"> ✔ (TF 2.x) </td>
<td style="text-align:center;"> ✔ </td>
<td style="text-align:center;"> ✔ </td>
</tr>
</tbody>
</table>
<p>There was also a minor issue with the benchmark that I noticed for Dask and Dagger that could possibly give them an unfair advantage over TF and JGE (which I've reported to Julius, who kindly updated their benchmark results). Specifically, the benchmarking script doesn’t wait on the launched computations to complete. This is a simple matter of calling <code>f2.compute()</code> and <code>fetch(f2)</code> for Dask and Dagger respectively, to force the execution of the full graph and the retrieval of the final result.</p>
<h3 id="benchmark_results" ><a href="#benchmark_results"> Benchmark Results</a></h3><p>For a quick comparison, I chose to briefly switch Dagger into AOT mode to get a better idea of how Dagger directly compared to JGE, and also how it compared with Dagger's JIT mode (Dask also added for comparison, and extra-long runs are excluded):</p>
<h4 id="comparison_initial_results_on_s_n_(in_seconds)" ><a href="#comparison_initial_results_on_s_n_(in_seconds)"> Comparison: Initial results on <code>s_n</code> (in seconds)</a></h4><table class="">
<thead>
<th style="text-align:left;"> # of Iterations </th>
<th style="text-align:center;"> Dagger AOT </th>
<th style="text-align:center;"> Dagger JIT </th>
<th style="text-align:center;"> Dask </th>
</thead>
<tbody>
<tr>
<td style="text-align:left;"> 1000 </td>
<td style="text-align:center;"> 34.108724 </td>
<td style="text-align:center;"> 4.031126 </td>
<td style="text-align:center;"> 1.6458556 </td>
</tr>
<tr>
<td style="text-align:left;"> 5000 </td>
<td style="text-align:center;"> ❌ </td>
<td style="text-align:center;"> 123.653653 </td>
<td style="text-align:center;"> 30.8954490 </td>
</tr>
<tr>
<td style="text-align:left;"> 10000 </td>
<td style="text-align:center;"> ❌ </td>
<td style="text-align:center;"> ❌ </td>
<td style="text-align:center;"> 136.261454 </td>
</tr>
</tbody>
</table>
<p>(An ❌ implies that the benchmark took too long to complete)</p>
<h4 id="comparison_initial_results_on_y_n_(in_seconds)" ><a href="#comparison_initial_results_on_y_n_(in_seconds)"> Comparison: Initial results on <code>y_n</code> (in seconds)</a></h4><table class="">
<thead>
<th style="text-align:left;"> # of Iterations </th>
<th style="text-align:center;"> Dagger AOT </th>
<th style="text-align:center;"> Dagger JIT </th>
<th style="text-align:center;"> Dask </th>
</thead>
<tbody>
<tr>
<td style="text-align:left;"> 1000 </td>
<td style="text-align:center;"> 0.136007 </td>
<td style="text-align:center;"> 3.378034 </td>
<td style="text-align:center;"> 1.5771625 </td>
</tr>
<tr>
<td style="text-align:left;"> 5000 </td>
<td style="text-align:center;"> 0.771038 </td>
<td style="text-align:center;"> 128.213972 </td>
<td style="text-align:center;"> 31.0315958 </td>
</tr>
<tr>
<td style="text-align:left;"> 10000 </td>
<td style="text-align:center;"> 1.184655 </td>
<td style="text-align:center;"> ❌ </td>
<td style="text-align:center;"> 133.501586 </td>
</tr>
<tr>
<td style="text-align:left;"> 100000 </td>
<td style="text-align:center;"> 13.019151 </td>
<td style="text-align:center;"> ❌ </td>
<td style="text-align:center;"> ❌ </td>
</tr>
<tr>
<td style="text-align:left;"> 200000 </td>
<td style="text-align:center;"> 27.982819 </td>
<td style="text-align:center;"> ❌ </td>
<td style="text-align:center;"> ❌ </td>
</tr>
<tr>
<td style="text-align:left;"> 500000 </td>
<td style="text-align:center;"> 73.965525 </td>
<td style="text-align:center;"> ❌ </td>
<td style="text-align:center;"> ❌ </td>
</tr>
</tbody>
</table>
<p>As we can see, AOT mode is <em>much</em> better than JIT mode on the <code>y_n</code> benchmark. AOT mode has some issues on the <code>s_n</code> benchmark, but that's due to splatting not being efficient at large scales in AOT mode (which is part of why I advise against using AOT mode). Still, regardless of the improvements from switching to AOT mode for <code>y_n</code>, I was disappointed by Dagger's performance in JIT mode, so I decided to continue investigating what I could do to improve that. The rest of this post will thus focus on Dagger's JIT mode.</p>
<h3 id="oh_no!_can_we_fix_it?" ><a href="#oh_no!_can_we_fix_it?"> Oh No! Can we fix it?</a></h3><p>Thankfully, the poor performance exhibited by Dagger is actually just the result of a lack of detailed optimizations in a select few (hot) code paths, which lead to slowdowns which dominate the majority of time that the benchmark was executing. Of course, all of these issues are now fixed on Dagger's <code>master</code> branch by the time this blog post reaches your eyes, but let's review what I fixed, just so you know that I'm not pulling a fast one on you.</p>
<p>First, how did I find out what was slowing things down? Easy answer (and if you've used Julia to do anything performance-sensitive, you can probably guess): <code>Profile.@profile</code>. The <code>Profile</code> stdlib uses a statistical profiler to help us find where in our code we're spending the most amount of time, and is immensely useful for finding hot code paths in Julia code<sup><a href="#fn_1">[1]</a></sup>
<a id="fnref_1"></a>
.</p>
<p>Ok, so we've got a way to see where and how our execution time was being spent; what did I actually find?</p>
<h3 id="first_fix_object_size_calculation" ><a href="#first_fix_object_size_calculation"> First Fix: Object Size Calculation</a></h3><p>Let's start with the most eggregious offender first: <code>Base.summarysize()</code>. This function is simple: it calculates approximately how much memory a given object takes, including every other object it directly or indirectly references. Unfortunately, it is also very slow; because it's recursive, it needs to be able to detect cyclic references, and handles every kind of object that could ever be passed to it with good latency. Furthering the unfortunate situation, our dependency package MemPool.jl calls this function every time a Dagger task produces a result (in <code>MemPool.poolset</code>, if you're wondering). If that happens many times, and/or if the objects passed in are somewhat large and complicated, then we'll see this function taking a large proportion of our runtime. And this was exactly what I saw; more than 37% of our runtime was spent here on the 1000-deep run, which is absolutely atrocious (and it gets worse as the depth grows).</p>
<p>So, what can we do about this? The specific case where this was occuring is in <code>add_thunk!</code>, which is where new tasks are generated and added to the scheduler. Here, thankfully, <code>MemPool.poolset</code> is being called on a small-ish task object for GC purposes; however, the size will not be used, because task objects can't be serialized over the network or to disk (the only two cases where size is used). To completely eliminate calls to <code>Base.summarysize()</code> when we don't want it called, we can just manually specify a size for the object being passed to <code>MemPool.poolset</code>, avoiding the <code>Base.summarysize()</code> call entirely. Therefore, we can safely pass any arbitrary size value to disable the automatic <code>Base.summarysize()</code> call.  With <a href="https://github.com/JuliaParallel/Dagger.jl/commit/2f47217c29e4ac9b2f0921df7bc18bdfe4356e2b">that change</a>, how do we fare?</p>
<table class="">
<thead>
<th style="text-align:left;"> # of Iterations </th>
<th style="text-align:center;"> Dagger JIT on <code>s_n</code> </th>
<th style="text-align:center;"> Dagger JIT on <code>y_n</code> </th>
</thead>
<tbody>
<tr>
<td style="text-align:left;"> 1000 </td>
<td style="text-align:center;"> 0.899850 </td>
<td style="text-align:center;"> 0.947113 </td>
</tr>
<tr>
<td style="text-align:left;"> 5000 </td>
<td style="text-align:center;"> 16.065269 </td>
<td style="text-align:center;"> 13.962618 </td>
</tr>
<tr>
<td style="text-align:left;"> 10000 </td>
<td style="text-align:center;"> 65.198173 </td>
<td style="text-align:center;"> 66.801668 </td>
</tr>
</tbody>
</table>
<p>Ok, that's much better! At 10000 depth, we shaved off about 50% from each benchmark! But we're still showing abysmal scaling, so what's next?</p>
<h3 id="second_fix_node_memoization" ><a href="#second_fix_node_memoization"> Second Fix: Node Memoization</a></h3><p>The next improvement came from how task dependencies are resolved. The <code>add_thunk!</code> function calls <code>reschedule_inputs!</code> to ensure that all "upstream" dependencies of a given task are satisfied before getting the task ready to be scheduled. While this function was recently optimized due to reported scaling issues, it's still far too slow, mostly because it recursively walks up the dependency chain until it finds that all upstream tasks are actively executing or finished executing. That's pretty silly; while not everything upstream is executing, that doesn't mean we need to keep walking through those tasks everytime we add a new task further down the DAG. What I chose to do was add a memoization dictionary to the scheduler that, when a task has been through <code>reschedule_inputs!</code> or an equivalent code path, holds an entry to that task to mark that it's not necessary to traverse it again. This was a <a href="https://github.com/JuliaParallel/Dagger.jl/commit/c17b86d13423351617c7a68ff2d5dafd27d7d32a">reasonably simple improvement</a>, trading a bit of memory for massively decreased execution overhead, leading us to these results:</p>
<table class="">
<thead>
<th style="text-align:left;"> # of Iterations </th>
<th style="text-align:center;"> Dagger JIT on <code>s_n</code> </th>
<th style="text-align:center;"> Dagger JIT on <code>y_n</code> </th>
</thead>
<tbody>
<tr>
<td style="text-align:left;"> 1000 </td>
<td style="text-align:center;"> 0.452218 </td>
<td style="text-align:center;"> 0.577350 </td>
</tr>
<tr>
<td style="text-align:left;"> 5000 </td>
<td style="text-align:center;"> 5.234815 </td>
<td style="text-align:center;"> 4.086071 </td>
</tr>
<tr>
<td style="text-align:left;"> 10000 </td>
<td style="text-align:center;"> 18.304120 </td>
<td style="text-align:center;"> 14.707820 </td>
</tr>
</tbody>
</table>
<p>Nice, we just cut out 72% of the <code>s_n</code> runtime and 78% of the <code>y_n</code> runtime. We're making good progress, but let's keep going!</p>
<h3 id="third_fix_domination_checks" ><a href="#third_fix_domination_checks"> Third Fix: Domination Checks</a></h3><p>Still looking at the same region of code, we find that we're spending a lot of runtime in validating that our graph is actually acyclic. More specifically, the <code>register_future!</code> function is called from <code>add_thunk!</code> to register a <code>Distributed.Future</code> that will be filled with the result of the newly-created task once it's done executing, allowing the user to wait on and fetch the task result. This function needs to be somewhat defensive, though, when being called from one task targetting another. If a task tries to register and wait on a future for some other task that is downstream of itself, it will wait forever, because that downstream task won't execute until the task waiting on it completes (thus, a deadlock occurs). Similarly, a task shouldn't be able to wait on itself. To avoid this, <code>register_future!</code> checks whether the calling task "dominates" the targetted task; when a task A dominates a task B, that means that the completion of A is necessary before the execution and completion of B can occur. If the calling task dominates the target task, then an error is thrown, preventing accidental deadlock. This check is well-intended, but is also slow; thankfully, when adding tasks with <code>add_thunk!</code>, we generally can assume that this new task isn't going to be waited on by a downstream task (it's possible, but a careful developer can trivially avoid it; we shouldn't burden them with unnecessary checks). To alleviate this, I simply added a kwarg to <code>register_future!</code> that will by default do the domination check, but can allow it to be manually disabled. For <code>@spawn</code>, which implicitly calls <code>add_thunk!</code>, we disable the check, because in common usage of that API it's not easy to cause deadlocks<sup><a href="#fn_2">[2]</a></sup>
<a id="fnref_2"></a>
. <a href="https://github.com/JuliaParallel/Dagger.jl/commit/d543c23815de79ae39616045aa1ee285665c014c">This change</a> gives us the following excellent results:</p>
<table class="">
<thead>
<th style="text-align:left;"> # of Iterations </th>
<th style="text-align:center;"> Dagger JIT on <code>s_n</code> </th>
<th style="text-align:center;"> Dagger JIT on <code>y_n</code> </th>
</thead>
<tbody>
<tr>
<td style="text-align:left;"> 1000 </td>
<td style="text-align:center;"> 0.201789 </td>
<td style="text-align:center;"> 0.223203 </td>
</tr>
<tr>
<td style="text-align:left;"> 5000 </td>
<td style="text-align:center;"> 1.216356 </td>
<td style="text-align:center;"> 1.173638 </td>
</tr>
<tr>
<td style="text-align:left;"> 10000 </td>
<td style="text-align:center;"> 2.711312 </td>
<td style="text-align:center;"> 2.428500 </td>
</tr>
<tr>
<td style="text-align:left;"> 100000 </td>
<td style="text-align:center;"> 25.743532 </td>
<td style="text-align:center;"> 28.761774 </td>
</tr>
<tr>
<td style="text-align:left;"> 200000 </td>
<td style="text-align:center;"> 59.582494 </td>
<td style="text-align:center;"> 64.312516 </td>
</tr>
<tr>
<td style="text-align:left;"> 500000 </td>
<td style="text-align:center;"> 201.391146 </td>
<td style="text-align:center;"> 225.147642 </td>
</tr>
</tbody>
</table>
<p>Wow, that's about 84% faster!</p>
<p>This is a good time to stop; trimming down everything else in the profile trace will likely require optimizations that fundamentally affect Dagger's semantics, which are waters that I don't want to wade through just to win a benchmark. With all of these changes in place, the final benchmark that I ran can be found at <a href="https://gist.github.com/jpsamaroo/95c78b3361ae454a51916183f2cf346f">this link</a> (and make sure to run with Dagger's <code>master</code> branch, where all these performance enhancements are now available!).</p>
<h3 id="giving_credit_where_credit_is_due" ><a href="#giving_credit_where_credit_is_due"> Giving credit where credit is due</a></h3><p>We've spent a lot of time discussing how Dagger can be made to compete better, but let's put that aside for a moment to be realistic and give credit where it's due; the work that Julius has done to make low/no-code programming both productive and performant in Julia (while expertly leveraging the many strengths of the language) is quite exceptional. The problem that their product is solving is one that us programmers often like to forget: programming is <em>hard</em> and it's <em>cumbersome</em>, and we all sometimes take that for granted when considering the best way for non-programmers to contribute their domain expertise to a business' success. It's easy to say, "Why don't you just learn to program?", but it's so much harder to actually learn even the bare basics (and yet more work to become proficient enough to make all this learning pay off). The Julius Graph Engine and its frontend UI environment cuts out the "cruft" of traditional programming, and lets domain experts do what they are lauded for without having to struggle on programming concepts that they didn't spend their entire schooling and careers training for.</p>
<p>I know many of us in the Julia community understand this plight, and most of us had to just endure the pain and struggle the struggle to get to the point where we could express our knowledge in the form that our favorite language demands it to be written. While it's not particularly helpful to ask "what if's" about what our future would have looked like if JGE had shown up a bit earlier, we can look toward the future and help Julius build out their product to provide the power of Julia's amazing ecosystem of packages in a form that everyone can enjoy.</p>
<h3 id="debrief" ><a href="#debrief"> Debrief</a></h3><p>Let's recap briefly what we've covered over the course of this post: I introduced Julius and their Graph Engine product, explained the basics of graph scheduling, showed off Julius' multi-faceted DAG benchmark, and walked you through how I optimized Dagger to bring our benchmark runtime down from "terrible" to "pretty damned good" through a few different optimizations:</p>
<ul>
<li>Avoiding automatic size calculations when object size is irrelevant</li>
<li>Using memoization to prevent re-walking sections of the DAG</li>
<li>Disabling graph cyclicity checks when unnecessary</li>
</ul>
<p>!!! note All of these changes are valid because we make certain simplifying assumptions about how code will be executed. If those assumptions stop holding, then we'll have to reconsider the correctness of these optimizations (which is quite similar to the question of when to use <code>@inbounds</code> in library code).</p>
<p>We also recognized that Julius' product offering is a powerful alternative to Dagger, especially for organizations which desire a low/no-code interface and strong performance on very large graphs (among other features).</p>
<h3 id="conclusion" ><a href="#conclusion"> Conclusion</a></h3><p>All of this leads us to a final question: what can Dagger do for you? Maybe you have a lot of images of different sizes that you want to shrink down to thumbnails, while utilizing GPUs where possible. Or you might have many matrices that need to have their eigenvalues calculated with an iterative approach, which can take differing amounts of time. If you're a data scientist, you may have large tables that need processing that you can split into chunks and process independently. You might be developing a SaaS application and need a way to execute "serverless" functions on event triggers.</p>
<h4 id="comparison_important_graph_scheduler_features" ><a href="#comparison_important_graph_scheduler_features"> Comparison: Important graph scheduler features</a></h4><table class="">
<thead>
<th style="text-align:left;"> Feature </th>
<th style="text-align:center;"> Dask </th>
<th style="text-align:center;"> Ray </th>
<th style="text-align:center;"> Dagger.jl </th>
<th style="text-align:center;"> JGE </th>
<th style="text-align:center;"> TensorFlow </th>
<th style="text-align:center;"> Cilk </th>
<th style="text-align:center;"> Vanilla Julia </th>
</thead>
<tbody>
<tr>
<td style="text-align:left;"> Multithreading </td>
<td style="text-align:center;"> ✔ </td>
<td style="text-align:center;"> ✔ </td>
<td style="text-align:center;"> ✔ </td>
<td style="text-align:center;"> ✔ </td>
<td style="text-align:center;"> ✔ </td>
<td style="text-align:center;"> ✔ </td>
<td style="text-align:center;"> ✔ </td>
</tr>
<tr>
<td style="text-align:left;"> Distributed </td>
<td style="text-align:center;"> ✔ </td>
<td style="text-align:center;"> ✔ </td>
<td style="text-align:center;"> ✔ </td>
<td style="text-align:center;"> ✔ </td>
<td style="text-align:center;">✔ </td>
<td style="text-align:center;"> ❌ </td>
<td style="text-align:center;"> ✔ </td>
</tr>
<tr>
<td style="text-align:left;"> GPUs    </td>
<td style="text-align:center;"> ✔ </td>
<td style="text-align:center;"> ✔ </td>
<td style="text-align:center;"> ✔ </td>
<td style="text-align:center;"> ❌ </td>
<td style="text-align:center;"> ✔ </td>
<td style="text-align:center;"> ❌ </td>
<td style="text-align:center;"> ✔ </td>
</tr>
<tr>
<td style="text-align:left;"> Mutability </td>
<td style="text-align:center;"> ❌ </td>
<td style="text-align:center;"> ✔ (Actors) </td>
<td style="text-align:center;"> ✔ (<code>@mutable</code>) </td>
<td style="text-align:center;"> ❌ </td>
<td style="text-align:center;"> ❌ </td>
<td style="text-align:center;"> ✔ </td>
<td style="text-align:center;"> ✔ </td>
</tr>
</tbody>
</table>
<p>There are so many possibilities, and Dagger strives to handle all of them efficiently.If your problem sounds even remotely similar, Dagger might be the right choice for you. If you aren't sure if Dagger will suit your needs, please reach out to me; my contact information is below!</p>
<h3 id="aside_future_work_and_collaboration" ><a href="#aside_future_work_and_collaboration"> Aside: Future work and collaboration</a></h3><p>I must admit, I wasn't sure whether Dagger was going to be able to compete with JGE's performance, but clearly we're now getting pretty close! Of course, there's still more work to do to bring down these times even further, but that can be left for another day and maybe for another contributor. Speaking of which: if this post has gotten you interested in contributing a bit to Dagger (even just some small things like adding some docs, tests, or examples), I'd love the help! Improvements like these aren't too hard to accomplish in an afternoon or two, but can make a huge difference for our users. If you decide that you'd like to help out, please drop me a line!</p>
<p>In the process of writing this post, I think I made it reasonably clear that graph schedulers are both simple yet simultaneously complicated beasts which rely on good performance engineering to get good runtime performance. Going forward, I'd like to cover other Dagger-related topics, such as the upcoming storage changes (aka "swap-to-disk"), and how to use Dagger and DaggerGPU for seamless GPU computing (among many other possible topics). If you have any ideas for a post that you'd like to read about, please message me with your thoughts!</p>
<h3 id="contact_information" ><a href="#contact_information"> Contact Information</a></h3><p><img src="/previews/PR40/assets/authors/julian-samaroo.png" alt="Julian Samaroo"  style="float: left; width:70px; padding: 0px 10px 0px 0px"/>

I'm <code>@jpsamaroo</code> on Slack, Zulip, or Discourse, and my email is jpsamaroo -AT- gmail -DOT- com. On Slack, Dagger questions are well-suited for the <code>#helpdesk</code>, <code>#multithreading</code> and <code>#distributed</code> channels.</p>
<br>


<br>    <script src="https://utteranc.es/client.js"
        repo="JuliaParallel/juliaparallel.github.io"
        issue-term="pathname"
        theme="github-light"
        crossorigin="anonymous"
        async>
    </script>

  

</div>


      </div> 
    </div>   

    <div class="page__footer">
      <footer>
        
        
        <div class="page__footer-follow">
          <ul class="social-icons">
            <li><strong>Follow:</strong></li>
            
            <li><a href="https://github.com/JuliaParallel" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
          </ul>
        </div>
        <div class="page__footer-copyright">&copy; JuliaParallel. Powered by <a href="https://github.com/tlienart/Franklin.jl">Franklin</a>, <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>, and <a href="https://julia.mit.edu" rel="nofollow">Julia Lab</a>.</div>
      </footer>
    </div>

    <script src="/previews/PR40/libs/minimal-mistakes/main.min.js"></script>
    <script defer src="https://use.fontawesome.com/releases/v5.8.2/js/all.js" integrity="sha384-DJ25uNYET2XCl5ZF++U8eNxPWqcKohUUBUpKGlNLMchM7q4Wjg2CUpjHLaL8yYPH" crossorigin="anonymous"></script>

    
    
        <script src="/previews/PR40/libs/highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: '    '});</script>

    
  </body>
</html>
